# Testing & Quality Assurance Standards

## Testing Framework

### Core Testing Stack
- **Framework**: pytest with asyncio support
- **Configuration**: [pyproject.toml](mdc:pyproject.toml) - pytest settings
- **Async Mode**: `asyncio_mode=auto` for automatic async test detection
- **Test Discovery**: [tests/](mdc:tests/) directory structure

### Test Organization
```
tests/
├── __init__.py
├── test_models.py              # Core model tests
├── test_base_agent.py          # Base agent tests
├── test_chroma_store.py        # Vector store tests
├── test_redis_messenger.py     # Message bus tests
└── conftest.py                 # Shared fixtures
```

## Test Structure Standards

### Test Function Naming
- **Prefix**: All test functions must start with `test_`
- **Descriptive**: Use descriptive names that explain what is being tested
- **Pattern**: `test_<function_name>_<scenario>`

```python
# Good test names
def test_agent_message_creation_with_valid_data():
    pass

def test_agent_message_creation_with_invalid_data():
    pass

def test_agent_message_creation_with_empty_content():
    pass
```

### Test File Organization
- **One Test File Per Module**: Test file should correspond to source module
- **Test Class Organization**: Group related tests in classes when beneficial
- **Import Organization**: Import test dependencies at the top

```python
# tests/test_models.py
import pytest
from src.models import AgentMessage, ExecutionResult, TaskSpecification

class TestAgentMessage:
    def test_creation_with_valid_data(self):
        # Test implementation
        pass
    
    def test_creation_with_invalid_data(self):
        # Test implementation
        pass

class TestExecutionResult:
    def test_success_result(self):
        # Test implementation
        pass
```

## Async Testing Standards

### Async Test Functions
- **Decorator**: Use `@pytest.mark.asyncio` for async tests
- **Async/Await**: Use proper async/await syntax
- **Error Handling**: Test both success and failure scenarios

```python
import pytest
from unittest.mock import AsyncMock

@pytest.mark.asyncio
async def test_agent_processes_message():
    agent = MyAgent()
    message = AgentMessage(message_type=MessageType.TASK_ASSIGNMENT)
    
    result = await agent.process_message(message)
    
    assert result.success is True
    assert result.message == "Message processed successfully"

@pytest.mark.asyncio
async def test_agent_handles_message_error():
    agent = MyAgent()
    message = AgentMessage(message_type=MessageType.INVALID_TYPE)
    
    result = await agent.process_message(message)
    
    assert result.success is False
    assert "Unknown message type" in result.message
```

### Async Fixtures
- **Async Fixtures**: Use async fixtures for async setup/teardown
- **Resource Management**: Properly manage async resources
- **Cleanup**: Ensure proper cleanup in async context

```python
@pytest.fixture
async def async_agent():
    agent = MyAgent()
    await agent.start()
    yield agent
    await agent.stop()

@pytest.fixture
async def mock_message_bus():
    bus = AsyncMock(spec=MessageBus)
    bus.publish = AsyncMock()
    bus.subscribe = AsyncMock()
    return bus
```

## Mocking Standards

### Mock Usage Guidelines
- **External Dependencies**: Mock all external services and APIs
- **Message Bus**: Mock Redis message bus for isolated testing
- **Vector Store**: Mock Chroma operations for unit tests
- **LLM Calls**: Mock AI model calls to avoid costs and external dependencies

### Mock Patterns
```python
from unittest.mock import AsyncMock, MagicMock, patch

# Mock external service
@pytest.mark.asyncio
async def test_agent_with_mocked_service():
    with patch('src.agents.my_agent.ExternalService') as mock_service:
        mock_service.return_value.call_api = AsyncMock(return_value="mocked_result")
        
        agent = MyAgent()
        result = await agent.execute_task(task)
        
        assert result.success is True
        assert result.data == "mocked_result"

# Mock message bus
@pytest.fixture
def mock_message_bus():
    bus = AsyncMock(spec=MessageBus)
    bus.publish = AsyncMock()
    bus.subscribe = AsyncMock()
    return bus

# Mock vector store
@pytest.fixture
def mock_vector_store():
    store = AsyncMock(spec=VectorStore)
    store.store_knowledge = AsyncMock()
    store.query_similar = AsyncMock(return_value=[])
    return store
```

## Assertion Standards

### Assertion Best Practices
- **Explicit Assertions**: Use explicit assertions, not implicit truthiness
- **Meaningful Messages**: Include descriptive assertion messages
- **Multiple Assertions**: Test multiple aspects when appropriate
- **Edge Cases**: Test boundary conditions and error scenarios

```python
# Good assertions
def test_execution_result_success():
    result = ExecutionResult.success("Task completed", {"data": "value"})
    
    assert result.success is True
    assert result.message == "Task completed"
    assert result.data == {"data": "value"}
    assert result.error is None

def test_execution_result_failure():
    result = ExecutionResult.failure("Task failed", "error_details")
    
    assert result.success is False
    assert result.message == "Task failed"
    assert result.error == "error_details"
    assert result.data is None

# Test edge cases
def test_agent_message_with_empty_content():
    message = AgentMessage(
        message_type=MessageType.TASK_ASSIGNMENT,
        content=""
    )
    
    assert message.content == ""
    assert len(message.content) == 0
    assert message.message_type == MessageType.TASK_ASSIGNMENT
```

## Test Data Management

### Test Data Creation
- **Factory Functions**: Create test data using factory functions
- **Realistic Data**: Use realistic but minimal test data
- **Variation**: Test with different data variations
- **Edge Cases**: Include boundary and invalid data

```python
# Test data factories
def create_valid_agent_message():
    return AgentMessage(
        id="test-123",
        message_type=MessageType.TASK_ASSIGNMENT,
        content={"task_id": "task-456", "priority": "high"},
        sender="coordinator",
        recipient="coder"
    )

def create_invalid_agent_message():
    return AgentMessage(
        id="",
        message_type="INVALID_TYPE",
        content=None,
        sender="",
        recipient=""
    )

# Test with variations
@pytest.mark.parametrize("priority", ["low", "medium", "high"])
def test_task_with_different_priorities(priority):
    task = TaskSpecification(
        id="test-task",
        priority=priority,
        context={"priority": priority}
    )
    
    assert task.priority == priority
    assert task.context["priority"] == priority
```

## Integration Testing

### Integration Test Guidelines
- **Service Integration**: Test interactions between components
- **End-to-End**: Test complete workflows when possible
- **Real Services**: Use real Redis/Chroma for integration tests
- **Isolation**: Ensure tests don't interfere with each other

```python
@pytest.mark.integration
@pytest.mark.asyncio
async def test_agent_message_flow():
    # Test complete message flow through system
    coordinator = CoordinatorAgent()
    coder = CoderAgent()
    
    # Send message
    message = create_valid_agent_message()
    await coordinator.send_message(message)
    
    # Verify message received
    received_messages = await coder.get_received_messages()
    assert len(received_messages) == 1
    assert received_messages[0].id == message.id
```

## Performance Testing

### Performance Test Standards
- **Response Time**: Test response times under load
- **Resource Usage**: Monitor memory and CPU usage
- **Scalability**: Test with increasing load
- **Baseline**: Establish performance baselines

```python
import time
import asyncio

@pytest.mark.performance
@pytest.mark.asyncio
async def test_agent_response_time():
    agent = MyAgent()
    start_time = time.time()
    
    # Execute multiple tasks concurrently
    tasks = [agent.execute_task(create_test_task()) for _ in range(10)]
    results = await asyncio.gather(*tasks)
    
    end_time = time.time()
    total_time = end_time - start_time
    
    # Assert performance requirements
    assert total_time < 1.0  # Should complete within 1 second
    assert all(result.success for result in results)
```

## Quality Metrics

### Test Coverage
- **Coverage Target**: Aim for >90% code coverage
- **Critical Paths**: Ensure critical business logic is fully tested
- **Edge Cases**: Test error conditions and boundary cases
- **Integration Points**: Test component interactions

### Code Quality
- **Linting**: All code must pass `ruff check --fix`
- **Type Checking**: All code must pass `mypy src --strict`
- **Documentation**: Public APIs must have docstrings
- **Performance**: No obvious performance issues

## Test Execution

### Running Tests
```bash
# Run all tests
uv run pytest -v

# Run specific test file
uv run pytest tests/test_models.py -v

# Run specific test function
uv run pytest tests/test_models.py::test_agent_message_creation -q

# Run tests with pattern matching
uv run pytest -k "agent and not redis" --maxfail=1

# Run tests with coverage
uv run pytest --cov=src --cov-report=html

# Run performance tests only
uv run pytest -m performance
```

### Test Categories
- **Unit Tests**: Test individual functions and classes
- **Integration Tests**: Test component interactions
- **Performance Tests**: Test performance characteristics
- **End-to-End Tests**: Test complete workflows

## Continuous Integration

### CI Requirements
- **Automated Testing**: All tests must pass in CI
- **Quality Gates**: Linting and type checking must pass
- **Coverage Reports**: Generate and track coverage metrics
- **Performance Monitoring**: Track performance regressions

### Pre-commit Checks
```bash
# Before committing, ensure:
ruff check --fix src tests
ruff format src tests
mypy src --strict
uv run pytest -v
```
description:
globs:
alwaysApply: false
---
